{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "Loading required package: linprog\n",
      "Loading required package: lpSolve\n"
     ]
    }
   ],
   "source": [
    "#start server in terminal with $python gym_http_server.py\n",
    "#install.packages(\"gym\")\n",
    "library(gym)\n",
    "#install.packages(\"MDPtoolbox\")\n",
    "library(MDPtoolbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remote_base \"http://127.0.0.1:5000\"\n",
    "rl_env_setup_func <- function(remote_base, env_name) {\n",
    "#start server in terminal with $python gym_http_server.py\n",
    "    remote_base <<- \"http://127.0.0.1:5000\"\n",
    "    client <<- create_GymClient(remote_base)\n",
    "    #print(client)\n",
    "    #create instance\n",
    "    instance_id <<- env_create(client, env_name)\n",
    "    #print(instance_id)\n",
    "    #list the environments\n",
    "    list_envs <<- env_list_all(client)\n",
    "    #print(list_envs)\n",
    "    #set up your agent\n",
    "    action_space_info <- env_action_space_info(client, instance_id)\n",
    "    #print(action_space_info)\n",
    "    agent <<- random_discrete_agent(action_space_info)\n",
    "    agent <<- random_discrete_agent(action_space_info[[\"n\"]])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdp_finite_horizon Solves finite-horizon MDP using backwards induction algorithm  \n",
    "**Description**  \n",
    "Solves finite-horizon MDP with backwards induction algorithm  \n",
    "**Usage**  \n",
    "    mdp_finite_horizon(P, R, discount, N, h)  \n",
    "**Arguments**  \n",
    "**P** transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].  \n",
    "**R** reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.  \n",
    "**discount** discount factor. discount is a real number which belongs to [0; 1[.  \n",
    "**N number of stages. N is an integer greater than 0.\n",
    "h (optional) terminal reward. h is a S length vector. By default, h = numeric(S)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param <- mdp_example_forest(S = 3, p = 0.1, r1 = 2, r2= 0)\n",
    "P <- param$P\n",
    "R <- param$R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh <- mdp_finite_horizon(P, R, discount = 0.9, N = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [,1] [,2] [,3] [,4]\n",
      "[1,] 1.3851 0.81    0    0\n",
      "[2,] 3.0051 1.62    1    0\n",
      "[3,] 5.0051 3.62    2    0\n",
      "     [,1] [,2] [,3]\n",
      "[1,]    1    1    1\n",
      "[2,]    1    1    2\n",
      "[3,]    1    1    1\n"
     ]
    }
   ],
   "source": [
    "print(fh$V)\n",
    "print(fh$policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
