{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration, Bellman Optimization, and Policy Iteration in R\n",
    "**Author:** Lauren Washington"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start server in terminal with $python gym_http_server.py\n",
    "#install.packages(\"gym\")\n",
    "library(gym)\n",
    "#install.packages(\"MDPtoolbox\")\n",
    "library(MDPtoolbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remote_base \"http://127.0.0.1:5000\"\n",
    "rl_env_setup_func <- function(remote_base, env_name) {\n",
    "#start server in terminal with $python gym_http_server.py\n",
    "    remote_base <<- \"http://127.0.0.1:5000\"\n",
    "    client <<- create_GymClient(remote_base)\n",
    "    #print(client)\n",
    "    #create instance\n",
    "    instance_id <<- env_create(client, env_name)\n",
    "    #print(instance_id)\n",
    "    #list the environments\n",
    "    list_envs <<- env_list_all(client)\n",
    "    #print(list_envs)\n",
    "    #set up your agent\n",
    "    action_space_info <- env_action_space_info(client, instance_id)\n",
    "    #print(action_space_info)\n",
    "    agent <<- random_discrete_agent(action_space_info)\n",
    "    agent <<- random_discrete_agent(action_space_info[[\"n\"]])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "\n",
    "mdp_value_iteration Solves discounted MDP using value iteration algorithm  \n",
    "**Description**  \n",
    "Solves discounted MDP with value iteration algorithm\n",
    "**Usage**  \n",
    "    mdp_value_iteration(P, R, discount, epsilon, max_iter, V0)\n",
    "**Arguments**  \n",
    "**P** transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].  \n",
    "**R** reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.  \n",
    "**discount** discount factor. discount is a real number which belongs to [0; 1[. For discount equals to 1, a warning recalls to check conditions of convergence.  \n",
    "**epsilon (optional) :** search for an epsilon-optimal policy. epsilon is a real in ]0; 1]. By default, epsilon = 0.01.\n",
    "**max_iter**\n",
    "**V0**\n",
    "**Details**\n",
    "**Value**    \n",
    "**policy**    \n",
    "**iter**\n",
    "**cpu_time**\n",
    "\n",
    "### Bellman Optimization\n",
    "\n",
    "mdp_bellman_operator(P, PR, discount, Vprev)  \n",
    "**Arguments**  \n",
    "**P** transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].  \n",
    "**PR** reward array. PR can be a 2 dimension array [S,A] possibly sparse. discount discount factor. discount is a real number belo**nging to ]0; 1]. Vprev value fonction. Vprev is a vector of length S.  \n",
    "**Details**  \n",
    "**Value**  \n",
    "**V** new value fonction. V is a vector of length S.  \n",
    "**policy** policy is a vector of length S. Each element is an integer corresponding to an action.  \n",
    "\n",
    "### Policy Iteration\n",
    "\n",
    "mdp_policy_iteration(P, R, discount, policy0, max_iter, eval_type)  \n",
    "**Arguments**  \n",
    "**P** transition probability array. P can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S].  \n",
    "**R** reward array. R can be a 3 dimensions array [S,S,A] or a list [[A]], each element containing a sparse matrix [S,S] or a 2 dimensional matrix [S,A] possibly sparse.  \n",
    "**discount** discount factor. discount is a real which belongs to ]0; 1[.  \n",
    "**policy0** (optional) starting policy. policy0 is a S length vector. By default, policy0 is the\n",
    "policy which maximizes the expected immediate reward.\n",
    "**max_iter** (optional) maximum number of iterations to be done. max_iter is an integer greater than 0. By default, max_iter is set to 1000.\n",
    "**eval_type** (optional) define function used to evaluate a policy. eval_type is 0 for mdp_eval_policy_matrix use, mdp_eval_policy_iterative is used in all other cases. By default, eval_type\n",
    "is set to 0.\n",
    "**Details**\n",
    "**Value**\n",
    "**V** optimal value fonction. V is a S length vector\n",
    "**policy** optimal policy. policy is a S length vector. Each element is an integer corre-\n",
    "sponding to an action which maximizes the value function iter number of iterations\n",
    "**cpu_time** CPU time used to run the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define value iteration function\n",
    "value_iteration_func <- function(S, p) {\n",
    "    param <- mdp_example_forest(S, p)\n",
    "    P <<- param$P\n",
    "    R <<- param$R\n",
    "    #run \n",
    "    vi <- mdp_value_iteration(P, R , 0.9)\n",
    "    V <<- vi$V\n",
    "    print(vi$V)\n",
    "    print(vi$policy)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define belman optimality function\n",
    "bellman_func <- function(Vprev) {\n",
    "    bellman_e <- mdp_bellman_operator(P, R, 0.9, Vprev = V)  \n",
    "    bellman_policy <<- bellman_e$policy\n",
    "    \n",
    "    print(bellman_e$V)\n",
    "    print(bellman_e$policy)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define policy iteration function\n",
    "policy_iteration_func <- function(policy0) {\n",
    "    pi <- mdp_policy_iteration(P, R , 0.9, policy0 = bellman_policy)\n",
    "    V <<- pi$V\n",
    "    policy <<- pi$policy\n",
    "    print(V)\n",
    "    print(policy)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"MDP Toolbox: iterations stopped, epsilon-optimal policy found\"\n",
      "[1] 5.148382 5.804728 6.616064\n",
      "[1] 1 1 2\n",
      "[1] 5.165184 5.822367 6.633544\n",
      "[1] 1 1 2\n",
      "[1] 5.320952 5.977860 6.788857\n",
      "[1] 1 1 2\n"
     ]
    }
   ],
   "source": [
    "#run env_setup_func\n",
    "rl_env_setup_func(\"http://127.0.0.1:5000\", \"Pendulum-v0\")\n",
    "\n",
    "#run value iteration, feed in optimal value into bellman optimization function, \n",
    "#feed in optimal policy into policy iteration function\n",
    "rl_models <- list(\n",
    "    value_iteration = value_iteration_func(S = 3, p = 0.1),\n",
    "    bellman_equation = bellman_func(Vprev),\n",
    "    policy_iteration = policy_iteration_func(policy0)\n",
    " )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
